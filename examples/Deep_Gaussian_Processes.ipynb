{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import RBFKernel, ScaleKernel\n",
    "from gpytorch.variational import WhitenedVariationalStrategy, CholeskyVariationalDistribution\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.mlls import VariationalELBO, AddedLossTerm\n",
    "from gpytorch.likelihoods import GaussianLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeKLDivergence(AddedLossTerm):\n",
    "    def __init__(self, variational_strategy):\n",
    "        self.variational_strategy = variational_strategy\n",
    "    \n",
    "    def loss(self):\n",
    "        return -1 * self.variational_strategy.kl_divergence().sum()\n",
    "\n",
    "class HiddenGPLayer(AbstractVariationalGP):\n",
    "    \"\"\"\n",
    "    Represents a hidden layer in a deep GP where inference is performed via the doubly stochastic method of \n",
    "    Salimbeni et al., 2017. Upon calling, instead of returning a variational distribution q(f), returns samples\n",
    "    from the variational distribution. \n",
    "    \n",
    "    See the documentation for __call__ below for more details below. Note that the behavior of __call__\n",
    "    will change to be much more elegant with multiple batch dimensions; however, the interface doesn't really\n",
    "    change.\n",
    "    \n",
    "    Args:\n",
    "        - input_dims (int): Dimensionality of input data expected by each GP\n",
    "        - output_dims (int): Number of GPs in this layer, equivalent to output dimensionality.\n",
    "        - num_inducing (int): Number of inducing points for this hidden layer\n",
    "        - num_samples (int): Number of samples to draw from q(f) for returning\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dims, output_dims, num_inducing=512, num_samples=20):\n",
    "        self.input_dims = input_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_samples = num_samples\n",
    "        inducing_points = torch.randn(output_dims, num_inducing, input_dims)\n",
    "        \n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_size=output_dims\n",
    "        )\n",
    "        variational_strategy = WhitenedVariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "        \n",
    "        super(HiddenGPLayer, self).__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = ConstantMean()\n",
    "        self.covar_module = ScaleKernel(RBFKernel(batch_size=output_dims), batch_size=output_dims)\n",
    "        \n",
    "        self.register_added_loss_term(\"hidden_kl_divergence\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward data through this hidden GP layer.\n",
    "        \n",
    "        If the input is 2 dimensional, we pass the input through each hidden GP, resulting in a `h x n` batch\n",
    "        Gaussian distribution. We then draw `s` samples from these Gaussians and reshape the result to be\n",
    "        `s x n x h` (e.g., h becomes the dimensionality of the output).\n",
    "        \n",
    "        If the input is 3 dimensional, we assume that the input is `s x n x d`, e.g. the batch dimension\n",
    "        corresponds to the number of samples. We use this as the number of samples to draw, and just propagate\n",
    "        each sample through the hidden layer. The output will be `s x n x h`. \n",
    "        \n",
    "        If the input is 4 dimensional, we assume that for some reason the user has already reshaped things to be\n",
    "        `h x s x n x d`. We reshape this internally to be `h x sn x d`, and the output will be `s x n x h`.\n",
    "        \n",
    "        The goal of these last two points is that if you have a tensor `x` that is `n x d`, then:\n",
    "            >>> hidden_gp2(hidden_gp(x))\n",
    "\n",
    "        will just work, and return a tensor of size `s x n x h2`, where `h2` is the output dimensionality of \n",
    "        hidden_gp2. In this way, hidden GP layers are easily composable.\n",
    "        \"\"\"\n",
    "        # TODO: Simplify this logic once multiple batch dimensions is supported\n",
    "        \n",
    "        if inputs.dim() == 2:\n",
    "            # Assume new input entirely\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            inputs = inputs.expand(self.output_dims, inputs.size(-2), self.input_dims)\n",
    "        elif inputs.dim() == 3:\n",
    "            # Assume batch dim is samples, not output_dim\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            inputs = inputs.expand(self.output_dims, inputs.size(1), inputs.size(-2), self.input_dims)\n",
    "\n",
    "        if inputs.dim() == 4:\n",
    "            num_samples = inputs.size(-3)\n",
    "            inputs = inputs.view(self.output_dims, inputs.size(-2) * inputs.size(-3), self.input_dims)\n",
    "            reshape_output = True\n",
    "        else:\n",
    "            reshape_output = False\n",
    "            num_samples = self.num_samples\n",
    "\n",
    "        variational_dist_f = super(HiddenGPLayer, self).__call__(inputs)\n",
    "        mean_qf = variational_dist_f.mean\n",
    "        std_qf = variational_dist_f.variance.sqrt()\n",
    "        \n",
    "        if reshape_output:\n",
    "            samples = torch.distributions.Normal(mean_qf, std_qf).rsample()\n",
    "            samples = samples.view(self.output_dims, num_samples, -1).permute(1, 2, 0)\n",
    "        else:\n",
    "            samples = torch.distributions.Normal(mean_qf, std_qf).rsample(torch.Size([num_samples]))\n",
    "            samples = samples.transpose(-2, -1)\n",
    "        \n",
    "        loss_term = NegativeKLDivergence(self.variational_strategy)\n",
    "        self.update_added_loss_term(\"hidden_kl_divergence\", loss_term)\n",
    "        \n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepGP(AbstractVariationalGP):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims, num_inducing=512, num_samples=20):\n",
    "        self.input_dims = input_dims\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.output_dims = output_dims\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "        inducing_points = torch.randn(output_dims, num_inducing, hidden_dims)\n",
    "\n",
    "        variational_distribution = CholeskyVariationalDistribution(\n",
    "            num_inducing_points=num_inducing,\n",
    "            batch_size=output_dims\n",
    "        )\n",
    "\n",
    "        variational_strategy = WhitenedVariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True\n",
    "        )\n",
    "        \n",
    "        super(DeepGP, self).__init__(variational_strategy)\n",
    "        \n",
    "        self.mean_module = ConstantMean(batch_size=output_dims)\n",
    "        self.covar_module = ScaleKernel(RBFKernel(batch_size=output_dims), batch_size=output_dims)\n",
    "        \n",
    "        # For more layers, just make more hidden layers to forward through\n",
    "        self.hidden_gp_layer = HiddenGPLayer(\n",
    "            input_dims=input_dims,\n",
    "            output_dims=hidden_dims,\n",
    "            num_inducing=num_inducing,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        The call method of a Deep GP differs from a standard variational GP simply in that we first\n",
    "        pass the data through any hidden GP layers, reshape the input to account for the fact that\n",
    "        the batch dimension should be interpreted as a number of samples, and then pass through the final layer.\n",
    "        \n",
    "        In some sense, the DeepGP object itself represents the last layer of the deep GP.\n",
    "        \"\"\"\n",
    "        # Forward through more layers here if they exist\n",
    "        hidden_inputs = self.hidden_gp_layer(inputs)\n",
    "        \n",
    "        # hidden_inputs is num_samples x n_inputs x hidden_dims\n",
    "        # Combine samples and inputs dimension since we want to mean over those in the likelihood\n",
    "        last_layer_inputs = hidden_inputs.contiguous().view(-1, self.hidden_dims)\n",
    "        last_layer_inputs = last_layer_inputs.unsqueeze(0).expand(self.output_dims, self.num_samples * inputs.size(-2), self.hidden_dims)\n",
    "        \n",
    "        return super(DeepGP, self).__call__(last_layer_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os.path\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "import numpy as np\n",
    "\n",
    "if not os.path.isfile('elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', 'elevators.mat')\n",
    "    \n",
    "data = torch.Tensor(loadmat('elevators.mat')['data'])\n",
    "X = data[:, :-1]\n",
    "y = data[:, -1]\n",
    "\n",
    "N = data.shape[0]\n",
    "np.random.seed(0)\n",
    "data = data[np.random.permutation(np.arange(N)),:]\n",
    "\n",
    "train_n = int(floor(0.8*len(X)))\n",
    "\n",
    "train_x = X[:train_n, :].contiguous().cuda()\n",
    "train_y = y[:train_n].contiguous().cuda()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous().cuda()\n",
    "test_y = y[train_n:].contiguous().cuda()\n",
    "\n",
    "mean = train_x.mean(dim=-2, keepdim=True)\n",
    "std = train_x.std(dim=-2, keepdim=True) + 1e-6\n",
    "train_x = (train_x - mean) / std\n",
    "test_x = (test_x - mean) / std\n",
    "\n",
    "mean,std = train_y.mean(),train_y.std()\n",
    "train_y = (train_y - mean) / std\n",
    "test_y = (test_y - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepGP(input_dims=18, hidden_dims=15, output_dims=1, num_inducing=512, num_samples=1).cuda()\n",
    "likelihood = GaussianLikelihood().cuda()\n",
    "mll = VariationalELBO(likelihood, model, train_x.size(-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 40\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.001)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    for minibatch_i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        train_y_reshaped = y_batch.unsqueeze(0).expand(model.num_samples, *y_batch.shape).contiguous().view(model.output_dims, model.num_samples * y_batch.size(-1))\n",
    "        loss = -mll(output, train_y_reshaped).sum()\n",
    "        \n",
    "        print('Epoch %d [%d/%d] - Loss: %.3f' % (i + 1, minibatch_i, len(train_loader), loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3520, device='cuda:0', grad_fn=<SqrtBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(torch.mean(torch.pow(preds.mean.reshape(model.num_samples, -1).mean(0) - test_y, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
